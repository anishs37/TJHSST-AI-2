# Unit 9: Neural Networks
In this unit, we go deeper into machine learning by learning about neural networks. First, we start with basic perceptron structures and applying them to problems like building a network to correctly model the XOR function. Then, we learn about gradient descent and how to find an optimal learning rate. Finally, we conclude with back propagation, and apply it to classifying digits in the MNIST dataset.

## Labs

The labs we covered in this unit, where the code for the labs are located in its respective subdirectory, are:
- Perceptrons
- Gradient Descent
    - Basic Gradient Descent
    - 1D Line Optimization
- Back Propagation
    - Back Propagation Brainteasers (backprop.py)
        - Model an example network
        - Model the sum function
        - Classify points as inside or outside the unit circle
    - MNIST digits classification
        - Back Propagation (mnist.py, mnist_test.py)
        - Back Propagation while distorting the training set (mnist_distortions.py, mnist_test_distortions.py)
        - Back Propagation with pairwise comparision networks (mnist_pairwise.py, mnist_test_pairwise.py)